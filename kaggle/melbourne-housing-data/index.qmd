---
title: Fun With Melbourne Housing Data
description: |
  Making some models and figures using Melbourne housing data from the 
  dataset [anthonypino/melbourne-housing-market](kaggle.com/datasets/anthonypino/melbourne-housing-market). 
  Notes from the first three assignments from ``dansbecker``s course and
  exposition.
image: err-dist-forest.png
extra:
  url: /kaggle/melbourne-housing-data
  image: /kaggle/melbourne-housing-data/err-dist-forest.png
keywords:
  - austrailia
  - seaborn
  - numpy 
  - sklearn
  - scikit-learn
  - datascience
  - data
  - science
catagories:
  - python
  - kaggle
date: 2024-08-27
date-modified: 2024-08-27
---

These notes/assignments were done along with ``dansbecker``s begginer course:

- [Basic Data Exploration](https://kaggle.com/code/dansbecker/basic-data-exploration)
- [Your First ML Model](https://kaggle.com/code/dansbecker/your-first-machine-learning-model)
- [Model Validation](https://www.kaggle.com/code/dansbecker/model-validation)
- [Underfitting and Overfitting](https://www.kaggle.com/code/dansbecker/underfitting-and-overfitting)


## Introduction

### Using ``kaggle`` Outside of the Browser

Since this is the first assignment, and since I would much rather automate 
things, I would like to say that it is worth knowing that the kaggle API has a
``python`` client available on ``PyPI``. This may be installed using ``pip 
install kaggle`` or in my case ``poetry add kaggle``. 

It turns out that the ``kaggle`` library is not the only client available for 
using ``kaggle`` in ``python`` modules. There is also a solution called 
``kagglehub``. It can be installed like ``poetry add kagglehub``.

The dataset for this assignment can be [viewed and downloaded in the browser](
https://www.kaggle.com/datasets/dansbecker/melbourne-housing-snapshot?resource=download).
It may be obtained in ``python`` as follows:


```{python}
from typing import Iterable, Type
import kagglehub
import pathlib
import io
import contextlib

import matplotlib.pyplot as plt
import seaborn as sb
import numpy as np
import pandas as pd

from IPython.display import display
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split


DIR = pathlib.Path(".").resolve()

# NOTE: The ``path`` argument does not specify the path downloaded to, but 
#       instead a subpath of the data.
DATA_DOWNLOAD_IO = io.StringIO()
DATA_ID ="anthonypino/melbourne-housing-market" 
with contextlib.redirect_stdout(DATA_DOWNLOAD_IO):
  DATA_DIR = pathlib.Path(kagglehub.dataset_download(DATA_ID))

DATA_PATH_LESS = DATA_DIR / "MELBOURNE_HOUSE_PRICES_LESS.csv"
DATA_PATH = DATA_DIR / "Melbourne_housing_FULL.csv"
```


### Loading and Describing Data

Note that it is necessary to capture ``stdout`` if you want your notebook to
look nice. The output in ``DATA_PATH`` should be a path to the data full data,
and (obviously) ``DATA_PATH_LESS`` should be a path to the partial data. It
will look something like


```{python}
#| echo: false
#| code-overflow: wrap
print(DATA_PATH)
```


Data is loaded an described using the following:


```{python}
#| fig-cap: Description of the dataset in ``MELBOURNE_HOUSE_PRICES_LESS.csv``.
DATA_LESS = pd.read_csv(DATA_PATH_LESS)
DATA_LESS.describe()
```


This is roughly what was done on the first assignment but with a different
data set (this one came from the example before) the homework assignment. 
Further the assignment asked for some interpretation of the data set 
description.


### Pandas Refresher

I will go ahead and write about pandas a little more as notes on the next 
tutorial and for my own review. 

The columns of the ``DataFrame`` are able to be viewed using the ``columns`` 
attribute:


```{python}
#| fig-cap: Available columns.
DATA = pd.read_csv(DATA_PATH)
print(
  "Columns:", 
  *list(map(lambda item: f"- `{item}`", DATA)),
  sep="\n"
)
```


``pd.core.series.Series`` is very similar to ``pd.DataFrame`` and shares many 
attributes. For instance, we can describe an individual column:


```{python}
#| fig-cap: Description of the ``Distance`` column.

DATA["Distance"].describe()
```


The following block of code confirms the type of ``DISTANCE`` and shows the 
useful attributes of ``pd.core.series.Series`` by filtering out methods and 
attributes that start with an underscore since they are often builtin or 
private:


```{python}
#| fig-cap: "Confirming that columns have type ``pd.core.series.Series``."
def describe_attrs(col):
  print("Type:", type(col))
  print(
    "Common Attributes:", 
    *list(
      map(
        lambda attr: f"- {attr}", 
        filter(
          lambda attr: not attr.startswith("_"), 
          set(dir(col)) & set(dir(DATA))
        )
      )
    ), 
    sep="\n",
  )

describe_attrs(DATA["Distance"])
```


Null columns can be removed from the ``DataFrame`` using the ``dropna`` method.
This does not modify in place the ``DataFrame``, rather it returns a new 
``DataFrame`` (unless the ``inplace`` keyword argument is used):


```{python}
#| fig-cap: Description of the data minus null rows.

def clean_data(data: pd.DataFrame):
  """Clean data and transform category columns into categories."""

  data_clean = data.dropna(axis='index')

  # NOTE: Categories are required for swarm plots.
  data_clean["Rooms"] = (rooms := data_clean["Rooms"]).astype(
    pd.CategoricalDtype(
      categories=list(range(rooms.min() - 1, rooms.max() + 1)),
      ordered=True,
    )
  )
  data_clean["Bathroom"] = (bathroom := data_clean["Bathroom"]).astype(
    pd.CategoricalDtype(
      categories=sorted(set(bathroom.dropna())),
      ordered=True,
    )
  )
  return data_clean

DATA_CLEAN = clean_data(DATA)
DATA_CLEAN_DESCRIPTION = DATA_CLEAN.describe() # We'll need this later.
display(DATA_CLEAN_DESCRIPTION)
```


The ``axis`` keyword argument of ``DataFrame.dropna`` is used to determine 
if rows (aka ``index`` or 0) or columns (``columns`` or 1) with null values are 
dropped. From this data a certain number of columns can be selected using a 
list as an index:


```{python}
#| fig-cap: First ten rows of ``DATA``.
DATA_FEATURES_COLUMNS = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']

# DATA_FEATURES: pd.DataFrame = DATA_CLEAN[DATA_FEATURES_COLUMNS] # type: ignore
# DATA_FEATURES.head(10)
#
# DATA_TARGET = DATA_CLEAN["Price"]
DATA_FEATURES: pd.DataFrame
TEST_FEATURES: pd.DataFrame
DATA_FEATURES, TEST_FEATURES, DATA_TARGET, TEST_TARGET = train_test_split( # type: ignore
  DATA_CLEAN[DATA_FEATURES_COLUMNS], 
  DATA_CLEAN["Price"],
  random_state=1,
)

DATA_FEATURES.head(10)
```

Note that ``sklearn.model_.train_test_split`` is used to chunk up the data so 
that we can compute error in predictions of the model outside of the data that 
will be used to train it, this is referred to as 'Out Sample' data.

'In Sample' data is used in the initial error analysis of the model used in 
this notebook. In the section after that, 'Out Sample' data is used to assess
the accuracy of the model. Finally, predictions are made for entries that 
did not have a price.


## Predicting Prices with a Tree Model

### About ``scikit-learn`` Learn

It is easy to install ``scikit-learn`` using poetry or pip like


```sh
poetry add scikit-learn
```


### Model Implementation

The following cell will predict the prices of houses for which the price is 
known:


```{python}
def create_model(features: pd.DataFrame, target, /, cls: Type =DecisionTreeRegressor, **kwargs):
  tree = cls(random_state=1, **kwargs)
  tree.fit(features, target)
  return tree

TREE = create_model(DATA_FEATURES, DATA_TARGET)
```


### Model In Sample Error Analysis

Now we should measure the accuracy of the model against some in sample data. 
This is done to contrast against our out sample analysis in the next section
of this notebook. The following function creates a dataframe for comparison:


```{python}
#| fig-cap: Predicted vs Actual Prices

def create_price_compare(
  tree,
  data: pd.DataFrame,
  *,
  price = None,
):
  """Create a dataframe with price, actual price, error, error_percent and 
  feature columns."""

  data_features = data[DATA_FEATURES_COLUMNS]
  price_actual = price if price is not None else data["Price"] 
  price_predictions = tree.predict(data_features)
  error = np.array(
    list(
      actual - predicted
      for predicted, actual in zip(price_predictions, price_actual)
    )
  )
  df = pd.DataFrame(
    {
      "predicted": price_predictions,
      "actual": price_actual, 
      "error": error,
      "error_percent": 100 * abs(error / price_actual)
    }
  )
  df = df.sort_values(by="error_percent")
  df = df.join(data_features)
  return df


PRICE_COMPARE = create_price_compare(TREE, DATA_CLEAN)
```


```{python}
#| echo: false
#| fig-cap: Description of ``PRICE_COMPARE["error_percent"]``.
PRICE_COMPARE["error_percent"].describe()
```




<!-- ```{python} -->
<!-- #| echo: false -->
<!-- def create_error_plots(): -->
<!--   sb.scatterplot( -->
<!--     PRICE_COMPARE[PRICE_COMPARE["error_percent"] < 500], -->
<!--     x="Longtitude", -->
<!--     y="Lattitude", -->
<!--     hue="error_percent", -->
<!--     alpha=0.5, -->
<!--     # ax=axs[0][1] -->
<!--   ) -->
<!---->
<!--   # TODO: These below lines should become a histogram -->
<!--   # sb.swarmplot( -->
<!--   #   PRICE_COMPARE[PRICE_COMPARE["Rooms"] < 6], # type: ignore -->
<!--   #   x="Rooms", -->
<!--   #   y="error_percent", -->
<!--   #   hue="actual", -->
<!--   #   alpha=0.5, -->
<!--   #   ax=axs[1][0] -->
<!--   # ) -->
<!---->
<!--   # sb.scatterplot( -->
<!--   #   PRICE_COMPARE, -->
<!--   #   x="Bathroom", -->
<!--   #   y="error_percent", -->
<!--   # -->
<!--   #   ax=axs[1][1], -->
<!--   # ) -->
<!--   plt.savefig("./errors.png") -->
<!--   plt.close() -->
<!---->
<!-- create_error_plots() -->
<!-- ``` -->
<!---->

<!-- ![Error Analysis Geospacial Plot.](./errors.png) -->


The description indicates that the mean error is reasonably low. Let's now plot 
the distribution of prediction errors within the in sample data:


```{python}
def create_model_errdist(price_compare: pd.DataFrame, *, ax=None, **kwargs):
  percents = np.linspace(0, 100, 50)
  counts = list(
    price_compare[
      (percent <= price_compare["error_percent"])
      & (price_compare["error_percent"] < percent + 2)
    ]["error_percent"].count()  # type: ignore
    for percent in percents
  )
  data = pd.DataFrame({"percents": percents, "counts": counts})
  return sb.lineplot(data, x="percents", y="counts", ax=ax, **kwargs)


err_dist = create_model_errdist(PRICE_COMPARE)
err_dist.set_title("Model Out Sample Error Distribution")
err_dist.figure.savefig("./err-dist-in-sample.png") # type: ignore
plt.close()
```


![In Sample Error Distribution](./err-dist-in-sample.png)



This is good (as most of the error is distributed between $0$ and $5$ percent).
However, as will be shown in the next section, this cannot be expected for any
out sample data.


### Model Out Sample Error Analysis

Conveniently, the functions above can be used for our out sample data. This is
as easy as


```{python}
#| fig-cap: Prediction analysis for out sample data.
TEST_PRICE_COMPARE = create_price_compare(TREE, TEST_FEATURES, price=TEST_TARGET)
TEST_PRICE_COMPARE["error_percent"].describe()
```


Nest, we can create a plot of the count per percent error:


```{python}
def mnae(price_compare: pd.DataFrame):
  cpc = abs(price_compare["error_percent"])
  return sum(cpc) / (100 * len(cpc)) # type: ignore


plt.axvline(mnae(TEST_PRICE_COMPARE) * 100)
err_dist = create_model_errdist(TEST_PRICE_COMPARE, color="red")
err_dist.set_title("Model Out Sample Error Distribution")
err_dist.figure.savefig(DIR / "err-dist-out-sample.png") # type: ignore
plt.close()
```


![Error Distribution](./err-dist-out-sample.png)


This plot does not look at all like the in sample error, which decays 
immediately with its spike contained under $5$ percent. It would indicate that 
error is generally high on the out sample data, implying that there is some
room for improvement. The vertical line is included to show the $mnae$, the 
mean normalized absolute error (as a percentage).


### Improving the Model

After running the model against some out sample data, it is clear that the 
model does not perform well right out of the box. If we were to only look at in
sample data, this would not be apparent.

It is possible to modify change model parameters to attempt to tune the model.
To make comparisons, we should combine the above script into a function to 
get the analysis dataframe for each tree.


```{python}
def create_tree_analysis(
  features: pd.DataFrame, 
  features_test: pd.DataFrame,
  target, 
  target_test,
  /,
  **kwargs
):
  tree = create_model(features, target, **kwargs)
  tree_price_compare = create_price_compare(tree, features_test, price=target_test)
  return tree, tree_price_compare
```


This function then can be mapped over many different parameter sets.


```{python}
def create_model_errdist_many(err_dists: list[pd.DataFrame], labels: Iterable[str]):
  fig, axs = plt.subplots(ncols=1, nrows=1)

  plot = None
  for dist, label in zip(err_dists, labels):
    plot = create_model_errdist(dist, ax=axs, label=label)

  assert plot is not None
  axs.legend()
  return plot


MAX_LEAF_NODES = list(map(lambda k: 5 * 10 ** k,  range(5)))
TREES, PRICE_COMPARES = zip(
  *(
    create_tree_analysis(
      DATA_FEATURES, TEST_FEATURES, DATA_TARGET, TEST_TARGET,
      max_leaf_nodes=max_leaf_nodes,
    )
    for max_leaf_nodes in MAX_LEAF_NODES
  )
)


err_dist = create_model_errdist_many(PRICE_COMPARES, map(str, MAX_LEAF_NODES))
err_dist.set_title("Model Out Sample Error for Various Models")
err_dist.figure.savefig("./err-dist-out-sample-many.png") # type: ignore
plt.close()
```


![Whatever](./err-many.png)



The best curves should have a strong peak towards the front (implying that 
error tends to lower for more entries) and decay rapidly. The initial model 
would appear to be reasonable fit because it matches the best curves (where 
``max_leaf_nodes`` is $5000$ and $50000$). 

It would appear that there is not much room for improvement of the model 
along parameter of ``max_leaf_nodes``. An objective choice of the number of 
leaf nodes can be done by minimizing the mean normalized absolute error:


```{python}
def min_mae(items, price_compares):
  maes = map( mnae, price_compares)
  best, candidate_mae = min(zip(items, maes), key = lambda pair: pair[1])
  return best, candidate_mae


BEST, BEST_MAE = min_mae(MAX_LEAF_NODES, PRICE_COMPARES)
print(f"The minimized mnae (`{BEST_MAE}`) has `max_leaf_nodes = {BEST}`.")
```

From this we will take the corresponding tree as the best model:


```{python}
TREE = TREES[MAX_LEAF_NODES.index(BEST)]
```


### Making Predictions with the Model 

The goal here is to plot and compare price preditions on the rows of ``DATA``
that did not have a price and make some pretty plots. Rows with null columns
can be found like follows:


```{python}
#| fig-cap: Description of the dataset rows with no price specified.

def create_price_null(data: pd.DataFrame) -> pd.DataFrame:
  price_null = data["Price"].isnull()
  return data[price_null][DATA_FEATURES_COLUMNS] # type: ignore

# DATA_FEATURES_PRICE_NULL: pd.DataFrame = DATA[DATA["Price"].isnull()][DATA_FEATURES_COLUMNS] # type: ignore
DATA_FEATURES_PRICE_NULL = create_price_null(DATA)
DATA_FEATURES_PRICE_NULL.describe()
```


This works because ``DATA["Price"]`` should contain the indices of the 
respective rows within the dataframe, making it a suitable index. In the 
description it is clear that this worked because the price stats are either 
zero of ``NaN``. Now it is time to attempt to fill in these values:


```{python}
#| fig-cap: Price predictions for the rows missing a price.

def create_price_predictions(tree: DecisionTreeRegressor, features: pd.DataFrame):
  predictions = tree.predict(features)
  completed = features.copy()
  completed["Price"] = predictions
  return completed


DATA_PRICE_NULL_PREDICTIONS = create_price_predictions(TREE, DATA_FEATURES_PRICE_NULL)
DATA_PRICE_NULL_PREDICTIONS.describe()
```


Note that ``TREE`` will reject the input if it contains all of columns and not
just the feature columns, thus why ``DATA_PRICE_NULL`` is indexed. The 
description of this dataframe should be reasonable comparable to the data
description of ``DATA_CLEAN``.


```{python}
#| fig-cap: Comparison of interpolated data and completed data descriptions. 

def create_price_predictions_compare(data_clean: pd.DataFrame, data_interpolated: pd.DataFrame):
  interpolated = data_interpolated["Price"].describe()
  actual = data_clean["Price"].describe()
  # error = interpolated - actual

  return pd.DataFrame(
    {
      "predicted": interpolated,
      "actual": actual,
      # "error": error,
      # "error_percent": 100 * (error / actual),
    }
  )


# NOTE: The last object in a code cell is displayed by default, thus why this
#       dataframe is created yet not assigned.
create_price_predictions_compare(DATA_CLEAN, DATA_PRICE_NULL_PREDICTIONS)
```


Now that we know the data descriptions are reasonable (by comparing the 
magnitude of any of the provided data) we can combine the predictions and
the clean data and label them as being estimated or not in the ``Estimated`` 
column. 


```{python}

def create_data_completed(data_clean: pd.DataFrame, data_interpolated: pd.DataFrame, ) -> pd.DataFrame:

  # NOTE: Create dataframe with features and prices, add that it is not estimated
  data_estimated_not = data_clean[[*DATA_FEATURES_COLUMNS, "Price"]].copy()
  data_estimated_not["Estimated"] = pd.Series(data=(False for _ in range(len(data_clean))))

  # NOTE: Add estimated to the estimated prices dataframe.
  data_interpolated = data_interpolated.copy()
  data_interpolated["Estimated"]= pd.Series(data=(True for _ in range(len(data_interpolated))))

  return pd.concat((data_estimated_not, data_interpolated)) # type: ignore


DATA_COMPLETED = create_data_completed(DATA_CLEAN, DATA_PRICE_NULL_PREDICTIONS)
```


This will allow us to generate some nice swarm plots in ``seaborn``.


```{python}

def create_prediction_plots(data_completed):
  price_min: float = data_completed["Price"].min() 
  price_ub = data_completed["Price"].quantile(0.99).max() 

  if not (_ := DIR / "rooms-stripplot.png").exists():
    rooms_plot = sb.swarmplot(
      data_completed[data_completed["Rooms"] <= 5], # noqa: reportArgumentType
      y="Price", 
      x="Rooms",
      hue="Estimated",
      dodge=True,
      size=0.99,
    )
    rooms_plot.set_xlim(0, 6)
    rooms_plot.set_ylim(price_min, price_ub)
    rooms_plot.figure.savefig(_) # type: ignore
    plt.close()

  if not (_ := DIR / "bathrooms-stripplot.png").exists():
    bathrooms_plot = sb.swarmplot(
      data_completed[data_completed["Bathroom"] <= 5], # noqa: reportArgumentType 
      y="Price", 
      x="Bathroom",
      hue="Estimated",
      dodge=True,
      size=0.99,
    )
    bathrooms_plot.set_xlim(0, 5)
    bathrooms_plot.set_ylim(price_min, price_ub)
    bathrooms_plot.figure.savefig(_) # type: ignore
    plt.close()

  # if not (_ := DIR / "geospacial-scatterplot.png").exists() or True:
  #   geospacial_plot = sb.scatterplot(
  #     DATA_COMPLETED,
  #     x="Longtitude",
  #     y="Lattitude",
  #     hue="Price",
  #     alpha=0.5,
  #   )
  #   # geospacial_plot = sb.histplot(
  #   #   DATA_COMPLETED,
  #   #   x=DATA_COMPLETED["Longtitude"],
  #   #   y=DATA_COMPLETED["Lattitude"],
  #   #   bins=25,
  #   #   cmap="mako",
  #   #   hue="Price"
  #   # )
  #
  #   geospacial_plot.figure.savefig(_)
  #   plt.close()

create_prediction_plots(DATA_COMPLETED)
```


![Swarm Plot by Rooms](./rooms-stripplot.png)

![Swarm Plot by Bathrooms.](./bathrooms-stripplot.png)

<!-- ![Geospacial Scatterplot](./geospacial-scatterplot.png) -->


It is interesting to notice the stacking of identical values on the prediction 
side. This would mean that the decision tree would follow a path down to the 
same node each time, an inherent problem with decision trees. In the next 
section an attempt to remedy this is made.


## Making Predictions With an Ensemble of Trees

A forest is simply many trees. To try to better the predictions made by a tree
and resolve over-fitting and under-fitting with some sort of consensus many trees
are used and their results are averaged. ``sklearn.ensemble.RandomForestRegressor``
may be constructed and trained in exactly the same way that ``DecisionTreeRegressor``
is, e.g.


```{python}
FOREST = create_model(DATA_FEATURES, DATA_TARGET, cls=RandomForestRegressor)
```


and now we may compare to some out sample data:


```{python}
PRICE_COMPARE = create_price_compare(FOREST, TEST_FEATURES, price=TEST_TARGET)
MNAE = mnae(PRICE_COMPARE) * 100

plt.axvline(MNAE)
err_dist = create_model_errdist(PRICE_COMPARE, color="red")
err_dist.set_title("Error Distribution of Random Forest")


err_dist.figure.savefig("./err-dist-forest.png") # type: ignore

plt.close()
```


![Random Forest Error Distribution](./err-dist-forest.png)


This leaves us with a mean normalize absolute error below $20$%, which is 
absolutely an improvement .
