---
title: Fun With Melbourne Housing Data
description: |
  Making some models and figures using Melbourne housing data from the 
  dataset [anthonypino/melbourne-housing-market](kaggle.com/datasets/anthonypino/melbourne-housing-market). 
  Notes from the first three assignments from ``dansbecker``s course and
  exposition.
image: thumb.png
extra:
  url: /kaggle/melbourne-housing-data
  image: /kaggle/melbourne-housing-data/thumb.png
keywords:
  - austrailia
  - seaborn
  - numpy 
  - sklearn
  - scikit-learn
  - datascience
  - data
  - science
catagories:
  - python
  - kaggle
date: 2024-08-27
date-modified: 2024-08-27
---

These notes/assignments were done along with the following parts of 
``dansbecker``s course:

- [Basic Data Exploration](https://kaggle.com/code/dansbecker/basic-data-exploration)
- [Your First ML Model](https://kaggle.com/code/dansbecker/your-first-machine-learning-model)
- [Model Validation](https://www.kaggle.com/code/dansbecker/model-validation)


## About Using ``kaggle`` Outside of the Browser

Since this is the first assignment, and since I would much rather automate 
things, I would like to say that it is worth knowing that the kaggle API has a
``python`` client available on ``PyPI``. This may be installed using ``pip 
install kaggle`` or in my case ``poetry add kaggle``. 

It turns out that the ``kaggle`` library is not the only client available for 
using ``kaggle`` in ``python`` modules. There is also a solution called 
``kagglehub``. It can be installed like ``poetry add kagglehub``.

The dataset for this assignment can be [viewed and downloaded in the browser](
https://www.kaggle.com/datasets/dansbecker/melbourne-housing-snapshot?resource=download).
It may be obtained in ``python`` as follows:


```{python}
from typing import Sequence
import kagglehub
import pathlib
import io
import contextlib

import matplotlib.pyplot as plt
import seaborn as sb
import numpy as np
import pandas as pd

from IPython.display import display


DIR = pathlib.Path(".").resolve()

# NOTE: The ``path`` argument does not specify the path downloaded to, but 
#       instead a subpath of the data.
DATA_DOWNLOAD_IO = io.StringIO()
DATA_ID ="anthonypino/melbourne-housing-market" 
with contextlib.redirect_stdout(DATA_DOWNLOAD_IO):
  DATA_DIR = pathlib.Path(kagglehub.dataset_download(DATA_ID))

DATA_PATH_LESS = DATA_DIR / "MELBOURNE_HOUSE_PRICES_LESS.csv"
DATA_PATH = DATA_DIR / "Melbourne_housing_FULL.csv"
```


## Describing Data

Note that it is necessary to capture ``stdout`` if you want your notebook to
look nice. The output in ``DATA_PATH`` should be a path to the data full data,
and (obviously) ``DATA_PATH_LESS`` should be a path to the partial data. It
will look something like


```{python}
#| echo: false
#| code-overflow: wrap
print(DATA_PATH)
```


Data is loaded an described using the following:


```{python}
#| fig-cap: Description of the dataset in ``MELBOURNE_HOUSE_PRICES_LESS.csv``.
DATA_LESS = pd.read_csv(DATA_PATH_LESS)
DATA_LESS.describe()
```


This is roughly what was done on the first assignment but with a different
data set (this one came from the example before) the homework assignment. 
Further the assignment asked for some interpretation of the data set 
description.


## More About Pandas

I will go ahead and write about pandas a little more as notes on the next 
tutorial and for my own review. 

The columns of the ``DataFrame`` are able to be viewed using the ``columns`` 
attribute:


```{python}
#| fig-cap: Available columns.
DATA = pd.read_csv(DATA_PATH)
print(
  "Columns:", 
  *list(map(lambda item: f"- `{item}`", DATA)),
  sep="\n"
)
```


``pd.core.series.Series`` is very similar to ``pd.DataFrame`` and shares many 
attributes. For instance, we can describe an individual column:


```{python}
#| fig-cap: Description of the ``Distance`` column.

DATA["Distance"].describe()
```


The following block of code confirms the type of ``DISTANCE`` and shows the 
useful attributes of ``pd.core.series.Series`` by filtering out methods and 
attributes that start with an underscore since they are often builtin or 
private:


```{python}
#| fig-cap: "Confirming that columns have type ``pd.core.series.Series``."
def describe_attrs(col):
  print("Type:", type(col))
  print(
    "Common Attributes:", 
    *list(
      map(
        lambda attr: f"- {attr}", 
        filter(
          lambda attr: not attr.startswith("_"), 
          set(dir(col)) & set(dir(DATA))
        )
      )
    ), 
    sep="\n",
  )

describe_attrs(DATA["Distance"])
```


Null columns can be removed from the ``DataFrame`` using the ``dropna`` method.
This does not modify in place the ``DataFrame``, rather it returns a new 
``DataFrame`` (unless the ``inplace`` keyword argument is used):


```{python}
#| fig-cap: Description of the data minus null rows.

def clean_data(data: pd.DataFrame):
  """Clean data and transform category columns into categories."""

  data_clean = data.dropna(axis='index')

  # NOTE: Categories are required for swarm plots.
  data_clean["Rooms"] = (rooms := data_clean["Rooms"]).astype(
    pd.CategoricalDtype(
      categories=list(range(rooms.min() - 1, rooms.max() + 1)),
      ordered=True,
    )
  )
  data_clean["Bathroom"] = (bathroom := data_clean["Bathroom"]).astype(
    pd.CategoricalDtype(
      categories=sorted(set(bathroom.dropna())),
      ordered=True,
    )
  )
  return data_clean

DATA_CLEAN = clean_data(DATA)
DATA_CLEAN_DESCRIPTION = DATA_CLEAN.describe() # We'll need this later.
display(DATA_CLEAN_DESCRIPTION)
```


The ``axis`` keyword argument of ``DataFrame.dropna`` is used to determine 
if rows (aka ``index`` or 0) or columns (``columns`` or 1) with null values are 
dropped. From this data a certain number of columns can be selected using a 
list as an index:


```{python}
#| fig-cap: First ten rows of ``DATA``.
DATA_FEATURES_COLUMNS = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']
DATA_FEATURES: pd.DataFrame = DATA_CLEAN[DATA_FEATURES_COLUMNS] # type: ignore
DATA_FEATURES.head(10)

DATA_TARGET = DATA_CLEAN["Price"]
```


## Predicting Prices with a Model

### About ``scikit-learn`` Learn

It is easy to install ``scikit-learn`` using poetry or pip like


```sh
poetry add scikit-learn
```



### Model Implementation

The following cell will predict the prices of houses for which the price is 
known:


```{python}
from sklearn.tree import DecisionTreeRegressor

def create_model(features: pd.DataFrame, target):
  tree = DecisionTreeRegressor(random_state=1)
  tree.fit(features, target)
  return tree

TREE = create_model(DATA_FEATURES, DATA_TARGET)
```


### Model Error Analysis

Now we should measure the accuracy of the model. The following function creates
a dataframe for comparison:


```{python}
#| fig-cap: Predicted vs Actual Prices

def create_price_compare(tree: DecisionTreeRegressor, data: pd.DataFrame):
  """Create a dataframe with price, actual price, error, error_percent and 
  feature columns."""

  data_features = data[DATA_FEATURES_COLUMNS]
  price_actual = data["Price"]
  price_predictions = tree.predict(data_features)
  error = np.array(
    list(
      actual - predicted
      for predicted, actual in zip(price_predictions, price_actual)
    )
  )
  df = pd.DataFrame(
    {
      "predicted": price_predictions,
      "actual": price_actual, 
      "error": error,
      "error_percent": 100 * abs(error / price_actual)
    }
  )
  df = df.sort_values(by="error_percent")
  df = df.join(data_features)
  return df


PRICE_COMPARE = create_price_compare(TREE, DATA_CLEAN)
```


```{python}
#| echo: false
#| fig-cap: Description of ``PRICE_COMPARE["error_percent"]``.
PRICE_COMPARE["error_percent"].describe()
```


The description indicates that the mean error is low. It is useful to know 
exactly how many houses have a prediction error exceeding some percent:


```{python}
#| fig-cap: Price comparison error analysis.
def create_price_compare_analysis(
  compare: pd.DataFrame, 
  percents: Sequence = (0.1, 0.5, 1, 2, 3, 5, 10, 20, 30)
):
  error_percent = compare["error_percent"]
  error_count = tuple(
    error_percent[error_percent > percent].count() # type: ignore
    for percent in percents
  )
  df = pd.DataFrame(
      {
        "error_count": error_count,
        "percent_dataset": 100 * (error_count/ error_percent.count()),
        "percent_minimum": percents
      }
  )
  return df


PRICE_COMPARE_ANALYSIS = create_price_compare_analysis(PRICE_COMPARE)
display(PRICE_COMPARE_ANALYSIS)
```


So the model predicts within 1% for about 98.2% of the data, and
within 0.1% for about 98% of the data, which is reasonable. However, in some 
cases the error is particularly bad. To see this, let's make a graph of actual
price vs prediction error.


```{python}
#| echo: false
def create_error_plots():
  sb.scatterplot(
    PRICE_COMPARE,
    x="Longtitude",
    y="Lattitude",
    hue="error_percent",
    alpha=0.5,
    # ax=axs[0][1]
  )

  # sb.swarmplot(
  #   PRICE_COMPARE[PRICE_COMPARE["Rooms"] < 6], # type: ignore
  #   x="Rooms",
  #   y="error_percent",
  #   hue="actual",
  #   alpha=0.5,
  #   ax=axs[1][0]
  # )

  # sb.scatterplot(
  #   PRICE_COMPARE,
  #   x="Bathroom",
  #   y="error_percent",
  #
  #   ax=axs[1][1],
  # )
  plt.savefig("./errors.png")
  plt.close()

create_error_plots()
```


![Error Analysis Geospacial Plot.](./errors.png)


### Making Predictions with the Model 

The goal here is to plot and compare price preditions on the rows of ``DATA``
that did not have a price and make some pretty plots. Rows with null columns
can be found like follows:


```{python}
#| fig-cap: Description of the dataset rows with no price specified.

def create_price_null(data: pd.DataFrame) -> pd.DataFrame:
  price_null = data["Price"].isnull()
  return data[price_null][DATA_FEATURES_COLUMNS] # type: ignore

# DATA_FEATURES_PRICE_NULL: pd.DataFrame = DATA[DATA["Price"].isnull()][DATA_FEATURES_COLUMNS] # type: ignore
DATA_FEATURES_PRICE_NULL = create_price_null(DATA)
DATA_FEATURES_PRICE_NULL.describe()
```


This works because ``DATA["Price"]`` should contain the indices of the 
respective rows within the dataframe, making it a suitable index. In the 
description it is clear that this worked because the price stats are either 
zero of ``NaN``. Now it is time to attempt to fill in these values:


```{python}
#| fig-cap: Price predictions for the rows missing a price.

def create_price_predictions(tree: DecisionTreeRegressor, features: pd.DataFrame):
  predictions = tree.predict(features)
  completed = features.copy()
  completed["Price"] = predictions
  return completed


DATA_PRICE_NULL_PREDICTIONS = create_price_predictions(TREE, DATA_FEATURES_PRICE_NULL)
DATA_PRICE_NULL_PREDICTIONS.describe()
```


Note that ``TREE`` will reject the input if it contains all of columns and not
just the feature columns, thus why ``DATA_PRICE_NULL`` is indexed. The 
description of this dataframe should be reasonable comparable to the data
description of ``DATA_CLEAN``.


```{python}
#| fig-cap: Comparison of interpolated data and completed data descriptions. 

def create_price_predictions_compare(data_clean: pd.DataFrame, data_interpolated: pd.DataFrame):
  interpolated = data_interpolated["Price"].describe()
  actual = data_clean["Price"].describe()
  # error = interpolated - actual

  return pd.DataFrame(
    {
      "predicted": interpolated,
      "actual": actual,
      # "error": error,
      # "error_percent": 100 * (error / actual),
    }
  )


# NOTE: The last object in a code cell is displayed by default, thus why this
#       dataframe is created yet not assigned.
create_price_predictions_compare(DATA_CLEAN, DATA_PRICE_NULL_PREDICTIONS)
```


Now that we know the data descriptions are reasonable (by comparing the 
magnitude of any of the provided data) we can combine the predictions and
the clean data and label them as being estimated or not in the ``Estimated`` 
column. 


```{python}

def create_data_completed(data_clean: pd.DataFrame, data_interpolated: pd.DataFrame, ) -> pd.DataFrame:

  # NOTE: Create dataframe with features and prices, add that it is not estimated
  data_estimated_not = data_clean[[*DATA_FEATURES_COLUMNS, "Price"]].copy()
  data_estimated_not["Estimated"] = pd.Series(data=(False for _ in range(len(data_clean))))

  # NOTE: Add estimated to the estimated prices dataframe.
  data_interpolated = data_interpolated.copy()
  data_interpolated["Estimated"]= pd.Series(data=(True for _ in range(len(data_interpolated))))

  return pd.concat((data_estimated_not, data_interpolated)) # type: ignore


DATA_COMPLETED = create_data_completed(DATA_CLEAN, DATA_PRICE_NULL_PREDICTIONS)
```


This will allow us to generate some nice swarm plots in ``seaborn``.


```{python}

def create_prediction_plots(data_completed):
  price_min: float = data_completed["Price"].min() 
  price_ub = data_completed["Price"].quantile(0.99).max() 

  if not (_ := DIR / "rooms-stripplot.png").exists():
    rooms_plot = sb.swarmplot(
      data_completed[data_completed["Rooms"] <= 5], # noqa: reportArgumentType
      y="Price", 
      x="Rooms",
      hue="Estimated",
      dodge=True,
      size=0.99,
    )
    rooms_plot.set_xlim(0, 6)
    rooms_plot.set_ylim(price_min, price_ub)
    rooms_plot.figure.savefig(_) # type: ignore
    plt.close()

  if not (_ := DIR / "bathrooms-stripplot.png").exists():
    bathrooms_plot = sb.swarmplot(
      data_completed[data_completed["Bathroom"] <= 5], # noqa: reportArgumentType 
      y="Price", 
      x="Bathroom",
      hue="Estimated",
      dodge=True,
      size=0.99,
    )
    bathrooms_plot.set_xlim(0, 5)
    bathrooms_plot.set_ylim(price_min, price_ub)
    bathrooms_plot.figure.savefig(_) # type: ignore
    plt.close()

  if not (_ := DIR / "geospacial-scatterplot.png").exists() or True:
    geospacial_plot = sb.scatterplot(
      DATA_COMPLETED,
      x="Longtitude",
      y="Lattitude",
      hue="Price",
      alpha=0.5,
    )
    # geospacial_plot = sb.histplot(
    #   DATA_COMPLETED,
    #   x=DATA_COMPLETED["Longtitude"],
    #   y=DATA_COMPLETED["Lattitude"],
    #   bins=25,
    #   cmap="mako",
    #   hue="Price"
    # )

    geospacial_plot.figure.savefig(_)
    plt.close()

create_prediction_plots(DATA_COMPLETED)
```


![Swarm Plot by Rooms](./rooms-stripplot.png)

![Swarm Plot by Bathrooms.](./bathrooms-stripplot.png)

![Geospacial Scatterplot](./geospacial-scatterplot.png)
